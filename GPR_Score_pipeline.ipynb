{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SciPy / sklearn\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import (\n",
    "    VarianceThreshold, SelectKBest, SelectFdr, f_classif, r_regression\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, roc_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# Optional: imbalanced-learn & mrmr\n",
    "try:\n",
    "    from imblearn.ensemble import EasyEnsembleClassifier\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except ImportError:\n",
    "    EasyEnsembleClassifier = None\n",
    "    SMOTE = None\n",
    "\n",
    "try:\n",
    "    import mrmr  # if not available, we fallback later\n",
    "except ImportError:\n",
    "    mrmr = None\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from joblib import dump\n",
    "from datetime import datetime\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b089c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_root: str, hk: bool = False) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Read CSV, process 'event', and return (features, label, patient_id).\"\"\"\n",
    "    data = pd.read_csv(file_root)\n",
    "    data = data.dropna(subset=['event'])\n",
    "    if hk:\n",
    "        data['event'] = data['event'].replace({'TRUE': True, '0': False})\n",
    "    if 'time' not in data.columns:\n",
    "        raise ValueError(\"'time' column is required for downstream analysis.\")\n",
    "    patient = data['ID']\n",
    "    label = data['event']\n",
    "    # Feature block (adjust if your features live elsewhere)\n",
    "    train_data = data.iloc[:, 1:754].copy()\n",
    "    for col in ['AGE', 'SEX', 'BMI']:\n",
    "        if col in data.columns:\n",
    "            train_data[col] = data[col]\n",
    "    return train_data, label, patient\n",
    "\n",
    "\n",
    "def confounding_correlation_reduction(feature_table: pd.DataFrame,\n",
    "                                      confounding_factor_table: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove features highly associated with confounding factors.\"\"\"\n",
    "    common_index = feature_table.dropna().index.intersection(confounding_factor_table.dropna().index)\n",
    "    for confounding_factor in confounding_factor_table.columns:\n",
    "        unique_values = confounding_factor_table[confounding_factor].unique()\n",
    "        if len(unique_values) < 5:\n",
    "            _, p_values = f_classif(\n",
    "                feature_table.loc[common_index].values,\n",
    "                confounding_factor_table.loc[common_index, confounding_factor].values\n",
    "            )\n",
    "            mask = p_values > 0.05\n",
    "        else:\n",
    "            r = r_regression(\n",
    "                feature_table.loc[common_index].values,\n",
    "                confounding_factor_table.loc[common_index, confounding_factor].values\n",
    "            )\n",
    "            mask = np.abs(r) < 0.65\n",
    "        feature_table = feature_table.loc[:, mask]\n",
    "    return feature_table\n",
    "\n",
    "\n",
    "def select_feature(train_data_collected: pd.DataFrame, para_factors: Dict) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Keyword exclusion + confounder filtering.\"\"\"\n",
    "    excluding_keywords = para_factors.get('Excluding keywords')\n",
    "    if excluding_keywords:\n",
    "        selected_features = [\n",
    "            name for name in train_data_collected.columns\n",
    "            if not any(keyword in name for keyword in excluding_keywords)\n",
    "        ]\n",
    "        selected_training_features = train_data_collected[selected_features].copy()\n",
    "        print(f\"{selected_training_features.shape[1]} features remain after keyword exclusion.\")\n",
    "    else:\n",
    "        selected_training_features = train_data_collected.copy()\n",
    "\n",
    "    confounding_factors = para_factors.get('Confounding factors')\n",
    "    if confounding_factors:\n",
    "        confounding_tables = [\n",
    "            train_data_collected[c] for c in confounding_factors if c in train_data_collected.columns\n",
    "        ]\n",
    "        if confounding_tables:\n",
    "            confounding_df = pd.concat(confounding_tables, axis=1)\n",
    "            selected_training_features = confounding_correlation_reduction(selected_training_features, confounding_df)\n",
    "            print(f\"{selected_training_features.shape[1]} features remain after confounder filtering.\")\n",
    "    return selected_training_features, list(selected_training_features.columns)\n",
    "\n",
    "\n",
    "def extract_pd_values(data_df: pd.DataFrame, label_pf: pd.Series) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    return data_df.values, label_pf.astype(int).values\n",
    "\n",
    "\n",
    "def classification_model_performance(model, predictors, ground_truth, return_roc=False):\n",
    "    probs = model.predict_proba(predictors)[:, 1]\n",
    "    preds = model.predict(predictors)\n",
    "    scores = {\n",
    "        'AUC': roc_auc_score(ground_truth, probs),\n",
    "        'Accuracy': accuracy_score(ground_truth, preds),\n",
    "        'Precision': precision_score(ground_truth, preds),\n",
    "        'Recall': recall_score(ground_truth, preds),\n",
    "    }\n",
    "    scores = pd.Series(scores)\n",
    "    if return_roc:\n",
    "        fpr, tpr, thresholds = roc_curve(ground_truth, probs)\n",
    "        roc_df = pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Threshold': thresholds})\n",
    "        return scores, roc_df\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrmr_names_or_fallback(X_df: pd.DataFrame, y: pd.Series, k: int) -> List[str]:\n",
    "    \"\"\"Return top-k feature names via mRMR; fallback to SelectKBest(f_classif) if not installed.\"\"\"\n",
    "    if mrmr is not None:\n",
    "        return mrmr.mrmr_classif(X=X_df, y=y, K=k)\n",
    "    print(\"[WARN] 'mrmr' not installed; falling back to SelectKBest(f_classif).\"\n",
    "          \" Results may differ from manuscript.\")\n",
    "    skb = SelectKBest(f_classif, k=min(k, X_df.shape[1]))\n",
    "    skb.fit(X_df.values, y.values)\n",
    "    return list(X_df.columns[skb.get_support()])\n",
    "\n",
    "\n",
    "def train_model(classifier: str, train_data, train_label):\n",
    "    classifier = classifier.upper()\n",
    "    model_map = {\n",
    "        'LR': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        'SVC': SVC(probability=True, random_state=RANDOM_STATE),\n",
    "        'RF': RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
    "        'GB': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        'NB': GaussianNB(),\n",
    "    }\n",
    "    if classifier not in model_map:\n",
    "        raise ValueError(f\"Unknown classifier '{classifier}'. Choose from LR, SVC, RF, GB, NB.\")\n",
    "    model = model_map[classifier]\n",
    "    model.fit(train_data, np.ravel(train_label))\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, test_label):\n",
    "    probs = model.predict_proba(test_data)[:, 1]\n",
    "    return roc_auc_score(np.ravel(test_label), probs)\n",
    "\n",
    "\n",
    "def print_results(mode_str, train_auc, val_auc, test_auc, evaluation_type):\n",
    "    print(f\"\\n[{evaluation_type}] Mode: {mode_str}\")\n",
    "    print(f\"Training AUC:   {train_auc:.4f}\" if train_auc is not None else \"Training AUC:   N/A\")\n",
    "    print(f\"Validation AUC: {val_auc:.4f}\"   if val_auc is not None else \"Validation AUC: N/A\")\n",
    "    print(f\"Test AUC:       {test_auc:.4f}\\n\" if test_auc is not None else \"Test AUC:       N/A\")\n",
    "\n",
    "\n",
    "def save_models(models: dict, out_dir: str | Path = \"artifacts/models\", prefix: str | None = None) -> dict:\n",
    "    \"\"\"Save trained sklearn models (.joblib).\"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tag = prefix if prefix is not None else datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    saved = {}\n",
    "    for name, model in models.items():\n",
    "        if model is None:\n",
    "            continue\n",
    "        fname = f\"{tag}_{name}.joblib\"\n",
    "        fpath = out_dir / fname\n",
    "        dump(model, fpath)\n",
    "        saved[name] = str(fpath)\n",
    "        print(f\"[saved] {name}: {fpath}\")\n",
    "    return saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config & constants -------------------------------------------------------\n",
    "DATA_ROOT = Path(\"data\")\n",
    "\n",
    "PATHS = {\n",
    "    'us_train': DATA_ROOT / 'us_train.csv',\n",
    "    'us_val':   DATA_ROOT / 'us_val.csv',\n",
    "    'us_test':  DATA_ROOT / 'us_test.csv',\n",
    "    'hk_train': DATA_ROOT / 'hk_train.csv',\n",
    "    'hk_val':   DATA_ROOT / 'hk_val.csv',\n",
    "    'hk_test':  DATA_ROOT / 'hk_test.csv',\n",
    "    'uk_test':  DATA_ROOT / 'uk_test.csv',   # optional external test\n",
    "}\n",
    "\n",
    "para_factors = {\n",
    "    'Confounding factors': ['original_shape2D_PixelSurface'],\n",
    "    'Excluding keywords': ['shape'],\n",
    "    'MRMR feature number': 24,\n",
    "}\n",
    "\n",
    "N_mrmr_ft = para_factors['MRMR feature number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US dataset (already merged) and apply filtering + mRMR\n",
    "us_train_data, us_train_label, us_train_patient = read_csv(str(PATHS['us_train']))\n",
    "us_val_data,   us_val_label,   us_val_patient   = read_csv(str(PATHS['us_val']))\n",
    "us_test_data,  us_test_label,  us_test_patient  = read_csv(str(PATHS['us_test']))\n",
    "\n",
    "us_train_data_hat, _ = select_feature(us_train_data, para_factors)\n",
    "us_mrmr_names = mrmr_names_or_fallback(us_train_data_hat, us_train_label, N_mrmr_ft)\n",
    "\n",
    "# Load HK dataset and apply filtering + mRMR\n",
    "hk_train_data, hk_train_label, hk_train_patient = read_csv(str(PATHS['hk_train']), hk=True)\n",
    "hk_val_data,   hk_val_label,   hk_val_patient   = read_csv(str(PATHS['hk_val']),   hk=True)\n",
    "hk_test_data,  hk_test_label,  hk_test_patient  = read_csv(str(PATHS['hk_test']),  hk=True)\n",
    "\n",
    "hk_train_data_hat, _ = select_feature(hk_train_data, para_factors)\n",
    "hk_mrmr_names = mrmr_names_or_fallback(hk_train_data_hat, hk_train_label, N_mrmr_ft)\n",
    "\n",
    "# Intersection as robust shared features\n",
    "shared_feat_names = list(set(us_mrmr_names).intersection(hk_mrmr_names))\n",
    "print(\"Number of intersected features selected by mRMR:\", len(shared_feat_names))\n",
    "print(\"US top (mRMR):\", us_mrmr_names)\n",
    "print(\"HK top (mRMR):\", hk_mrmr_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35af124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_np(Xdf, yser):\n",
    "    return extract_pd_values(Xdf, yser)\n",
    "\n",
    "# US own mRMR set\n",
    "us_train_X = us_train_data[us_mrmr_names];  us_train_X, us_train_y = _to_np(us_train_X, us_train_label)\n",
    "us_val_X   = us_val_data[us_mrmr_names];    us_val_X,   us_val_y   = _to_np(us_val_X,   us_val_label)\n",
    "us_test_X  = us_test_data[us_mrmr_names];   us_test_X,  us_test_y  = _to_np(us_test_X,  us_test_label)\n",
    "\n",
    "# HK own mRMR set\n",
    "hk_train_X = hk_train_data[hk_mrmr_names];  hk_train_X, hk_train_y = _to_np(hk_train_X, hk_train_label)\n",
    "hk_val_X   = hk_val_data[hk_mrmr_names];    hk_val_X,   hk_val_y   = _to_np(hk_val_X,   hk_val_label)\n",
    "hk_test_X  = hk_test_data[hk_mrmr_names];   hk_test_X,  hk_test_y  = _to_np(hk_test_X,  hk_test_label)\n",
    "\n",
    "# Shared feature view (for combined model)\n",
    "us_shared_train_X = us_train_data[shared_feat_names]; us_shared_train_X, us_shared_train_y = _to_np(us_shared_train_X, us_train_label)\n",
    "us_shared_val_X   = us_val_data[shared_feat_names];   us_shared_val_X,   us_shared_val_y   = _to_np(us_shared_val_X,   us_val_label)\n",
    "us_shared_test_X  = us_test_data[shared_feat_names];  us_shared_test_X,  us_shared_test_y  = _to_np(us_shared_test_X,  us_test_label)\n",
    "\n",
    "hk_shared_train_X = hk_train_data[shared_feat_names]; hk_shared_train_X, hk_shared_train_y = _to_np(hk_shared_train_X, hk_train_label)\n",
    "hk_shared_val_X   = hk_val_data[shared_feat_names];   hk_shared_val_X,   hk_shared_val_y   = _to_np(hk_shared_val_X,   hk_val_label)\n",
    "hk_shared_test_X  = hk_test_data[shared_feat_names];  hk_shared_test_X,  hk_shared_test_y  = _to_np(hk_shared_test_X,  hk_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30632da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'GB'  # choose from: LR, SVC, RF, GB, NB\n",
    "\n",
    "# Train within-domain models (own mRMR sets)\n",
    "model_us = train_model(classifier, us_train_X, us_train_y)\n",
    "train_auc_us = evaluate_model(model_us, us_train_X, us_train_y)\n",
    "val_auc_us   = evaluate_model(model_us, us_val_X,   us_val_y)\n",
    "test_auc_us  = evaluate_model(model_us, us_test_X,  us_test_y)\n",
    "print_results(\"us -> us\", train_auc_us, val_auc_us, test_auc_us, \"Initial Evaluation\")\n",
    "\n",
    "model_hk = train_model(classifier, hk_train_X, hk_train_y)\n",
    "train_auc_hk = evaluate_model(model_hk, hk_train_X, hk_train_y)\n",
    "val_auc_hk   = evaluate_model(model_hk, hk_val_X,   hk_val_y)\n",
    "test_auc_hk  = evaluate_model(model_hk, hk_test_X,  hk_test_y)\n",
    "print_results(\"hk -> hk\", train_auc_hk, val_auc_hk, test_auc_hk, \"Initial Evaluation\")\n",
    "\n",
    "# Cross-dataset (own mRMR sets)\n",
    "test_auc_us_hk = evaluate_model(model_us, hk_test_X, hk_test_y)\n",
    "print_results(\"us -> hk\", train_auc_us, val_auc_us, test_auc_us_hk, \"Cross-Dataset Evaluation\")\n",
    "\n",
    "test_auc_hk_us = evaluate_model(model_hk, us_test_X, us_test_y)\n",
    "print_results(\"hk -> us\", train_auc_hk, val_auc_hk, test_auc_hk_us, \"Cross-Dataset Evaluation\")\n",
    "\n",
    "# Combined/shared-features model trained on US shared features, then tested on each domain's shared test\n",
    "model_combined = train_model(classifier, us_shared_train_X, us_shared_train_y)\n",
    "test_auc_combined_us = evaluate_model(model_combined, us_shared_test_X, us_shared_test_y)\n",
    "print_results(\"Combined (shared) -> us\", None, None, test_auc_combined_us, \"Expected Outcome (After Proposed Solution)\")\n",
    "\n",
    "test_auc_combined_hk = evaluate_model(model_combined, hk_shared_test_X, hk_shared_test_y)\n",
    "print_results(\"Combined (shared) -> hk\", None, None, test_auc_combined_hk, \"Expected Outcome (After Proposed Solution)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: UK external test set (if present)\n",
    "if PATHS['uk_test'].exists():\n",
    "    uk_test_data, uk_test_label, uk_test_patient = read_csv(str(PATHS['uk_test']), hk=True)\n",
    "\n",
    "    # Shared features with UK\n",
    "    uk_shared_test_X = uk_test_data[shared_feat_names]; uk_shared_test_X, uk_shared_test_y = extract_pd_values(uk_shared_test_X, uk_test_label)\n",
    "\n",
    "    # US/HK own mRMR projections for UK (only if columns exist); guard with set intersection\n",
    "    us_uk_cols = [c for c in us_mrmr_names if c in uk_test_data.columns]\n",
    "    hk_uk_cols = [c for c in hk_mrmr_names if c in uk_test_data.columns]\n",
    "\n",
    "    if us_uk_cols:\n",
    "        ukus_test_X = uk_test_data[us_uk_cols]; ukus_test_X, ukus_test_y = extract_pd_values(ukus_test_X, uk_test_label)\n",
    "        test_auc_ukus = evaluate_model(model_us, ukus_test_X, ukus_test_y)\n",
    "        print_results(\"us -> uk\", None, None, test_auc_ukus, \"Initial Evaluation\")\n",
    "    else:\n",
    "        print(\"[INFO] No overlapping columns for US mRMR set in UK test; skipping us->uk.\")\n",
    "\n",
    "    if hk_uk_cols:\n",
    "        ukhk_test_X = uk_test_data[hk_uk_cols]; ukhk_test_X, ukhk_test_y = extract_pd_values(ukhk_test_X, uk_test_label)\n",
    "        test_auc_ukhk = evaluate_model(model_hk, ukhk_test_X, ukhk_test_y)\n",
    "        print_results(\"hk -> uk\", None, None, test_auc_ukhk, \"Initial Evaluation\")\n",
    "    else:\n",
    "        print(\"[INFO] No overlapping columns for HK mRMR set in UK test; skipping hk->uk.\")\n",
    "\n",
    "    test_auc_combined_uk = evaluate_model(model_combined, uk_shared_test_X, uk_shared_test_y)\n",
    "    print_results(\"Combined (shared) -> uk\", None, None, test_auc_combined_uk, \"Expected Outcome (After Proposed Solution)\")\n",
    "else:\n",
    "    print(\"[INFO] PATHS['uk_test'] not found; skipping UK external evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models (joblib). Change keys as you prefer.\n",
    "saved_paths = save_models(\n",
    "    models={\n",
    "        'us': model_us,\n",
    "        'hk': model_hk,\n",
    "        'combined': model_combined,\n",
    "    },\n",
    "    out_dir='models'\n",
    ")\n",
    "saved_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
